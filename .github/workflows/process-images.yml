name: Process Images

on:
  workflow_dispatch:
    inputs:
      test_mode:
        description: 'Run in test mode with sample data'
        required: false
        default: false
        type: boolean
  workflow_call:
    inputs:
      image_urls_json:
        description: 'JSON string containing image URLs data'
        required: true
        type: string
      repository:
        description: 'Repository to update with processed images'
        required: true
        type: string
    secrets:
      token:
        description: 'GitHub token with write access to target repository'
        required: true

jobs:
  process-images:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout image-utils repository
      uses: actions/checkout@v4
    
    - name: Checkout cian-tracker data
      uses: actions/checkout@v4
      with:
        repository: klimmm/cian-tracker
        sparse-checkout: |
          data/images/
          data/cian_data/image_urls.json
        sparse-checkout-cone-mode: false
        path: cian-tracker-data
        token: ${{ secrets.CROSS_REPO_TOKEN || secrets.token || secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install tensorflow pillow numpy scikit-learn imagehash
    
    - name: Check for data to process
      id: check_data
      run: |
        if '${{ github.event_name }}' == 'workflow_dispatch' && '${{ inputs.test_mode }}' == 'true'; then
          echo "has_data=true" >> $GITHUB_OUTPUT
          echo "Running in test mode with sample data"
        elif '${{ github.event_name }}' == 'workflow_call'; then
          echo "has_data=true" >> $GITHUB_OUTPUT
          echo "Processing data from workflow call"
        elif [ -f "cian-tracker-data/data/cian_data/image_urls.json" ]; then
          echo "has_data=true" >> $GITHUB_OUTPUT
          echo "Image URLs file found, proceeding with image processing"
        else
          echo "has_data=false" >> $GITHUB_OUTPUT
          echo "No image URLs file found, skipping image processing"
        fi

    - name: Process images
      if: steps.check_data.outputs.has_data == 'true'
      env:
        PYTHONPATH: ${{ github.workspace }}
      run: |
        echo "üñºÔ∏è Starting image download and processing..."
        python -c "
        import sys
        import os
        sys.path.insert(0, os.getcwd())
        
        from image_filter import prefilter_listings_for_download
        from download_images import download_images
        from image_dedup import dedupe_images
        from predict_image import predict_images_batch
        from load_model import load_model
        import json
        
        # Handle test mode or real data
        if '${{ github.event_name }}' == 'workflow_dispatch' and '${{ inputs.test_mode }}' == 'true':
            # Use sample test data
            image_urls_data = [
                {
                    'offer_id': 'test_001',
                    'image_urls': [
                        'https://via.placeholder.com/400x300?text=Test+Image+1',
                        'https://via.placeholder.com/400x300?text=Test+Image+2'
                    ]
                }
            ]
            print('Running in test mode with sample data')
        elif '${{ github.event_name }}' == 'workflow_call':
            # Parse input data from workflow_call
            image_urls_data = json.loads('${{ inputs.image_urls_json }}')
        else:
            # Read from cian-tracker data file
            image_urls_file = 'cian-tracker-data/data/cian_data/image_urls.json'
            with open(image_urls_file, 'r', encoding='utf-8') as f:
                image_urls_data = json.load(f)
        
        # Convert comma-separated image_urls strings to arrays
        for listing in image_urls_data:
            if 'image_urls' in listing and isinstance(listing['image_urls'], str):
                # Split comma-separated string into array and clean up
                urls = [url.strip() for url in listing['image_urls'].split(',') if url.strip()]
                listing['image_urls'] = urls
                
        print(f'üìä Loaded {len(image_urls_data)} listings with image URLs')
        
        # Configuration
        image_dir = 'temp_images'
        model_path = 'best_model.h5'
        
        # Create images directory if it doesn't exist
        os.makedirs(image_dir, exist_ok=True)
        
        # Check if we have existing images from cian-tracker
        existing_images_dir = 'cian-tracker-data/data/images'
        if os.path.exists(existing_images_dir):
            print(f'Found existing images directory: {existing_images_dir}')
            # Copy existing images to processing directory
            import shutil
            for item in os.listdir(existing_images_dir):
                src = os.path.join(existing_images_dir, item)
                dst = os.path.join('temp_images', item)
                if os.path.isfile(src):
                    shutil.copy2(src, dst)
                elif os.path.isdir(src):
                    shutil.copytree(src, dst, dirs_exist_ok=True)
            print(f'Copied existing images to temp_images directory')
        
        try:
            if not image_urls_data:
                print('‚úÖ No listings with image URLs found')
                exit(0)
                
            print(f'üìã Processing {len(image_urls_data)} listings with image URLs')
            
            # Step 1: Pre-filter listings to avoid unnecessary downloads
            print('üßπ Pre-filtering listings for download...')
            filtered_listings = prefilter_listings_for_download(image_urls_data, image_dir)
            print(f'üìä Filtered to {len(filtered_listings)} listings requiring download (from {len(image_urls_data)} total)')
            
            # Step 2: Download images (only for filtered listings)
            if filtered_listings:
                print('üñºÔ∏è Starting image download process...')
                download_images(filtered_listings, image_dir)
                print('‚úÖ Image download completed!')
            else:
                print('‚ÑπÔ∏è No listings require image downloads - all images already present')
            
            # Step 3: Deduplicate images
            print('üßπ Starting image deduplication...')
            for listing in image_urls_data:
                offer_id = str(listing.get('offer_id'))
                offer_dir = os.path.join(image_dir, offer_id)
                if os.path.exists(offer_dir):
                    from image_dedup import dedupe_images
                    dedupe_images(offer_dir, hash_size=8, max_distance=0)
            print('‚úÖ Image deduplication completed!')
            
            # Step 4: Run ML predictions if model exists
            print('ü§ñ Starting ML predictions on images...')
            if os.path.exists(model_path):
                model = load_model(model_path)
                if model is not None:
                    stats = predict_images_batch(image_urls_data, image_dir, model, max_workers=4)
                    print(f'‚úÖ ML predictions completed: {stats[\"total_processed\"]} images processed')
                else:
                    print('‚ö†Ô∏è Model not loaded, skipping predictions')
            else:
                print('‚ö†Ô∏è Model file not found, skipping predictions')
                
            print('‚úÖ Image processing pipeline completed successfully!')
            
            # Save results for artifact upload
            results = {
                'total_listings': len(image_urls_data),
                'filtered_listings': len(filtered_listings),
                'status': 'completed'
            }
            with open('processing_results.json', 'w') as f:
                json.dump(results, f)
            
        except Exception as e:
            print(f'‚ùå Error during image processing: {e}')
            import traceback
            traceback.print_exc()
            exit(1)
        "
    
    - name: Upload results
      uses: actions/upload-artifact@v4
      with:
        name: image-processing-results
        path: |
          processing_results.json
          temp_images/
        retention-days: 7
    
    - name: Push processed images back to cian-tracker
      if: steps.check_data.outputs.has_data == 'true'
      run: |
        cd cian-tracker-data
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Copy processed images back
        cp -r ../temp_images/* data/images/
        
        # Check if there are changes and commit
        if [ -n "$(git status --porcelain data/images/)" ]; then
          echo "üìÅ Adding processed images to git..."
          git add data/images/
          
          # Create commit message with timestamp
          TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')
          COMMIT_MSG="Auto-process images on $TIMESTAMP

          ü§ñ Generated with [Claude Code](https://claude.ai/code)

          Co-Authored-By: Claude <noreply@anthropic.com>"
          
          echo "üíæ Committing changes..."
          git commit -m "$COMMIT_MSG"
          
          echo "üöÄ Pushing to remote repository with retry logic..."
          
          # Retry push with pull if needed
          MAX_RETRIES=3
          RETRY_COUNT=0
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if git push; then
              echo "‚úÖ Images committed and pushed successfully"
              break
            else
              echo "‚ö†Ô∏è Push failed (attempt $((RETRY_COUNT + 1))/$MAX_RETRIES)"
              
              if [ $RETRY_COUNT -lt $((MAX_RETRIES - 1)) ]; then
                echo "üîÑ Pulling latest changes and retrying..."
                
                # Stash any unstaged changes
                git stash push -m "Temporary stash before pull" || true
                
                # Pull with rebase
                if git pull --rebase origin main; then
                  echo "‚úÖ Successfully pulled and rebased changes"
                  
                  # Pop stashed changes back if any
                  git stash pop || true
                  
                  echo "‚è≥ Retrying push in 5 seconds..."
                  sleep 5
                else
                  echo "‚ùå Failed to pull and rebase"
                  break
                fi
              fi
              
              RETRY_COUNT=$((RETRY_COUNT + 1))
            fi
          done
          
          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
            echo "‚ùå Failed to push after $MAX_RETRIES attempts"
            exit 1
          fi
        else
          echo "‚ÑπÔ∏è No new images to commit"
        fi
    
    - name: Summary
      if: always()
      run: |
        echo "üèÅ Image processing workflow completed"
        if [ "${{ steps.check_data.outputs.has_data }}" = "true" ]; then
          echo "‚úÖ Data was processed"
          if [ -f "processing_results.json" ]; then
            echo "üìã Results file created successfully"
            echo "üìä Processed images count: $(find temp_images -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" 2>/dev/null | wc -l)"
          else
            echo "‚ö†Ô∏è No results file found"
          fi
        else
          echo "‚ÑπÔ∏è No data to process"
        fi
